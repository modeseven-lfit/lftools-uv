# SPDX-License-Identifier: EPL-1.0
##############################################################################
# Copyright (c) 2025 The Linux Foundation and others.
#
# All rights reserved. This program and the accompanying materials
# are made available under the terms of the Eclipse Public License v1.0
# which accompanies this distribution, and is available at
# http://www.eclipse.org/legal/epl-v10.html
##############################################################################
"""Test Unicode handling across the codebase."""

import logging

import pytest


class TestUnicodeInLogging:
    """Test that logging handles Unicode characters correctly."""

    def test_logging_with_fstring_unicode(self, caplog):
        """Test that f-string logging works with Unicode characters."""
        log = logging.getLogger(__name__)

        # Test various Unicode characters in f-strings
        test_cases = [
            ("Ani≈õ Be≈Çur", "team-√±ame"),
            ("Se√±or Garc√≠a", "espa√±ol"),
            ("Fran√ßois M√ºller", "fran√ßais"),
            ("–í–ª–∞–¥–∏–º–∏—Ä –ü–µ—Ç—Ä–æ–≤", "—Ä—É—Å—Å–∫–∏–π"),
            ("Áî∞‰∏≠Â§™ÈÉé", "Êó•Êú¨Ë™û"),
            ("Jo≈æe ≈†mit", "sloven≈°ƒçina"),
        ]

        with caplog.at_level(logging.INFO):
            for name, team in test_cases:
                # This should NOT raise UnicodeEncodeError
                log.info(f"User {name} from team {team}")
                assert name in caplog.text
                assert team in caplog.text

    def test_logging_with_percent_formatting_unicode(self, caplog):
        """Test that percent-style logging works with Unicode characters."""
        log = logging.getLogger(__name__)

        with caplog.at_level(logging.INFO):
            # Percent-style formatting should handle Unicode
            log.info("Creating repo under organization: %s", "t√´am-√±ame")
            assert "t√´am-√±ame" in caplog.text

            log.info("User %s has voted on change %s", "Ani≈õ Be≈Çur", "73947")
            assert "Ani≈õ Be≈Çur" in caplog.text

    def test_logging_error_with_unicode(self, caplog):
        """Test that error logging handles Unicode correctly."""
        log = logging.getLogger(__name__)

        with caplog.at_level(logging.ERROR):
            log.error("Failed to process user: Jo≈æe ≈†mit")
            assert "Jo≈æe ≈†mit" in caplog.text

    def test_logging_with_unicode_in_dict(self, caplog):
        """Test logging with Unicode characters in dictionary values."""
        log = logging.getLogger(__name__)

        data = {"user": "Fran√ßois", "organization": "Fondation Linux", "team": "√©quipe-d√©veloppement"}

        with caplog.at_level(logging.INFO):
            log.info(f"Processing data: {data}")
            assert "Fran√ßois" in caplog.text
            assert "√©quipe-d√©veloppement" in caplog.text

    def test_incorrect_logging_syntax_detection(self):
        """Test that we can detect incorrect logging syntax patterns."""
        # These are examples of INCORRECT patterns that should be avoided
        incorrect_patterns = [
            # Using comma instead of f-string or %
            'log.info("Creating repo: ", org_name)',
            'log.info("User:", username)',
            'log.info("Approvals:", approval_list)',
        ]

        # Correct patterns
        correct_patterns = [
            'log.info(f"Creating repo: {org_name}")',
            'log.info("User: %s", username)',
            'log.info(f"Approvals: {approval_list}")',
        ]

        # This test serves as documentation of correct vs incorrect patterns
        assert len(incorrect_patterns) > 0
        assert len(correct_patterns) == len(incorrect_patterns)


class TestUnicodeInStrings:
    """Test Unicode handling in string operations."""

    def test_unicode_characters_in_json_strings(self):
        """Test that JSON strings with Unicode are handled correctly."""
        import json

        test_data = {"user": "Ani≈õ Be≈Çur", "group": "t√´am-√±ame", "description": "Project with espa√±ol: √±√°√©√≠√≥√∫"}

        # Should not raise UnicodeEncodeError
        json_str = json.dumps(test_data, ensure_ascii=False)
        assert "Ani≈õ Be≈Çur" in json_str
        assert "t√´am-√±ame" in json_str

        # Should be able to encode to UTF-8
        encoded = json_str.encode("utf-8")
        assert isinstance(encoded, bytes)

        # Should be able to decode back
        decoded = encoded.decode("utf-8")
        assert decoded == json_str

    def test_unicode_in_url_parameters(self):
        """Test that Unicode in URL parameters is handled."""
        import urllib.parse

        # Test encoding Unicode characters for URL
        params = {"user": "Jo≈æe ≈†mit", "team": "√©quipe-fran√ßaise"}

        # Should not raise UnicodeEncodeError
        encoded = urllib.parse.urlencode(params)
        assert "Jo" in encoded  # The ≈æ will be percent-encoded
        assert "quipe" in encoded  # The √© will be percent-encoded

    def test_unicode_characters_from_different_scripts(self):
        """Test handling of Unicode from various writing systems."""
        unicode_strings = [
            "Latin: Ani≈õ Be≈Çur",
            "Greek: ŒëŒªŒ≠ŒæŒ±ŒΩŒ¥œÅŒøœÇ",
            "Cyrillic: –í–ª–∞–¥–∏–º–∏—Ä",
            "Hebrew: ◊©◊ú◊ï◊ù",
            "Arabic: ŸÖÿ±ÿ≠ÿ®ÿß",
            "Japanese: „Åì„Çì„Å´„Å°„ÅØ",
            "Korean: ÏïàÎÖïÌïòÏÑ∏Ïöî",
            "Thai: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ",
            "Emoji: üëãüåç",
        ]

        for test_str in unicode_strings:
            # Should encode and decode without errors
            encoded = test_str.encode("utf-8")
            decoded = encoded.decode("utf-8")
            assert decoded == test_str

    def test_unicode_normalization(self):
        """Test Unicode normalization handling."""
        import unicodedata

        # Same character, different representations
        # √© can be: 1) single character U+00E9, or 2) e + combining acute U+0065 U+0301
        str1 = "Fran√ßois"  # Composed form
        str2 = "Fran√ßois"  # May be decomposed form

        # Normalize both to NFC (composed)
        normalized1 = unicodedata.normalize("NFC", str1)
        normalized2 = unicodedata.normalize("NFC", str2)

        # Should be equal after normalization
        assert normalized1 == normalized2


class TestUnicodeEdgeCases:
    """Test edge cases and specific characters that have caused issues."""

    def test_latin1_incompatible_characters(self):
        """Test characters that cannot be encoded with latin-1."""
        # These characters caused the original bug: 'latin-1' codec can't encode
        problematic_chars = [
            "\u0161",  # ≈° (lowercase s with caron)
            "\u0141",  # ≈Å (uppercase L with stroke)
            "\u0142",  # ≈Ç (lowercase l with stroke)
            "\u0107",  # ƒá (lowercase c with acute)
            "\u010d",  # ƒç (lowercase c with caron)
            "\u017e",  # ≈æ (lowercase z with caron)
        ]

        for char in problematic_chars:
            test_string = f"User name: Josip {char}ivkovic"

            # Should encode with UTF-8 without error
            encoded = test_string.encode("utf-8")
            assert isinstance(encoded, bytes)

            # Should fail with latin-1 (this is the original bug)
            with pytest.raises(UnicodeEncodeError):
                test_string.encode("latin-1")

    def test_emoji_handling(self):
        """Test that emojis are handled correctly."""
        emoji_string = "Success! ‚úÖ User üë§ logged in from üåç"

        # Should encode to UTF-8
        encoded = emoji_string.encode("utf-8")
        decoded = encoded.decode("utf-8")
        assert decoded == emoji_string

    def test_zero_width_characters(self):
        """Test handling of zero-width Unicode characters."""
        # Zero-width space, zero-width joiner, etc.
        zwsp = "\u200b"  # Zero-width space
        zwj = "\u200d"  # Zero-width joiner

        test_string = f"Name{zwsp}with{zwj}special{zwsp}chars"

        # Should handle these characters
        encoded = test_string.encode("utf-8")
        decoded = encoded.decode("utf-8")
        assert decoded == test_string

    def test_combining_diacritical_marks(self):
        """Test combining diacritical marks."""
        # Base character + combining mark
        base = "e"
        acute = "\u0301"  # Combining acute accent
        combined = base + acute  # Should display as √©

        # Should encode/decode correctly
        encoded = combined.encode("utf-8")
        decoded = encoded.decode("utf-8")
        assert decoded == combined


class TestUnicodeInGerritScenarios:
    """Test Unicode handling in Gerrit-specific scenarios."""

    def test_gerrit_user_names_with_unicode(self):
        """Test that Gerrit user names with Unicode are handled."""
        # Common European names that might appear in Gerrit
        gerrit_users = [
            "Ani≈õ Be≈Çur",
            "Fran√ßois Dupont",
            "Bj√∂rn M√ºller",
            "Jos√© Garc√≠a",
            "≈Åukasz Kowalski",
            "Jo≈æe ≈†mit",
        ]

        for user in gerrit_users:
            # Simulate JSON payload that would be sent to Gerrit API
            import json

            payload = json.dumps({"reviewer": user}, ensure_ascii=False)

            # Should encode to UTF-8 for API request
            encoded = payload.encode("utf-8")
            assert isinstance(encoded, bytes)

            # Should decode back correctly
            decoded = encoded.decode("utf-8")
            assert user in decoded

    def test_gerrit_group_names_with_unicode(self):
        """Test that Gerrit group names with Unicode are handled."""
        group_names = [
            "t√´am-√±ame",
            "√©quipe-d√©veloppement",
            "–∫–æ–º–∞–Ω–¥–∞-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤",
        ]

        for group in group_names:
            import json

            payload = json.dumps({"group": group}, ensure_ascii=False)
            encoded = payload.encode("utf-8")
            decoded = encoded.decode("utf-8")
            assert group in decoded

    def test_gerrit_commit_messages_with_unicode(self):
        """Test that commit messages with Unicode are handled."""
        commit_messages = [
            "Fix: Resolved issue reported by Fran√ßois",
            "Feature: Added support for espa√±ol",
            "Doc: Updated README with Êó•Êú¨Ë™û translation",
        ]

        for message in commit_messages:
            # Should encode/decode without issues
            encoded = message.encode("utf-8")
            decoded = encoded.decode("utf-8")
            assert decoded == message


class TestUnicodeInGitHubScenarios:
    """Test Unicode handling in GitHub-specific scenarios."""

    def test_github_repo_descriptions_with_unicode(self):
        """Test that repository descriptions with Unicode are handled."""
        descriptions = [
            "A project for fran√ßais developers",
            "Herramienta para desarrolladores en espa√±ol",
            "Êó•Êú¨Ë™û„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà",
        ]

        for desc in descriptions:
            # Should handle Unicode in API calls
            encoded = desc.encode("utf-8")
            decoded = encoded.decode("utf-8")
            assert decoded == desc

    def test_github_team_names_with_unicode(self, caplog):
        """Test that team names with Unicode are handled."""
        log = logging.getLogger(__name__)
        org_name = "test-org"
        team_name = "√©quipe-fran√ßaise"

        with caplog.at_level(logging.INFO):
            # This simulates the logging that happens in create-team
            # Should NOT raise TypeError
            log.info(f"Creating team {team_name} under organization {org_name}")
            assert team_name in caplog.text
            assert org_name in caplog.text


class TestUnicodeRegressionPrevention:
    """Tests specifically designed to prevent regressions of the fixed bugs."""

    def test_prevent_latin1_encoding_error_regression(self):
        """Ensure the latin-1 encoding error doesn't regress.

        This test specifically targets the bug fixed in:
        - lftools Gerrit change #73947
        - lftools-uv fix in api/client.py
        """
        # The character that caused the original bug
        problematic_string = '{"user": "Jo≈æe ≈†mit"}'  # Contains \u0161

        # This should work with UTF-8 encoding
        encoded = problematic_string.encode("utf-8")
        assert isinstance(encoded, bytes)

        # Verify the fix prevents latin-1 encoding issues
        # If we accidentally use latin-1, this would fail
        with pytest.raises(UnicodeEncodeError):
            problematic_string.encode("latin-1")

    def test_prevent_logging_format_error_regression(self, caplog):
        """Ensure the logging format error doesn't regress.

        This test specifically targets the bug fixed in:
        - lftools Gerrit change #73947 (github_cli.py)
        - lftools-uv fix in cli/github_cli.py and api/endpoints/gerrit.py
        """
        log = logging.getLogger(__name__)

        org_name = "test-org"
        approval_list = ["user1", "user2"]

        with caplog.at_level(logging.INFO):
            # Correct f-string format (should work)
            log.info(f"Creating repo under organization: {org_name}")
            assert "Creating repo under organization: test-org" in caplog.text

            # Correct f-string format (should work)
            log.info(f"Approvals: {approval_list}")
            assert "Approvals:" in caplog.text

    def test_unicode_in_api_request_data(self):
        """Test that API request data with Unicode is properly handled.

        This simulates what happens in RestApi._request() after the fix.
        """
        # Simulate the fix in client.py
        data = '{"reviewer": "Ani≈õ Be≈Çur", "group": "t√´am-√±ame"}'

        # The fix: encode string data as UTF-8
        if isinstance(data, str):
            data = data.encode("utf-8")

        # Verify it's now bytes
        assert isinstance(data, bytes)

        # Verify it can be decoded back correctly
        decoded = data.decode("utf-8")
        assert "Ani≈õ Be≈Çur" in decoded
        assert "t√´am-√±ame" in decoded


class TestUnicodeInNexusScenarios:
    """Test Unicode handling in Nexus-specific scenarios."""

    def test_nexus_json_encoding_with_unicode(self):
        """Test that Nexus JSON data with Unicode is encoded as UTF-8, not latin-1.

        This test specifically targets the bug found in lftools_uv/nexus/__init__.py
        where json.dumps().encode(encoding="latin-1") was used in 5 places.
        """
        import json

        # Test data that might contain Unicode (e.g., user names, descriptions)
        test_cases = [
            {"name": "Fran√ßois Dupont", "email": "francois@example.com"},
            {"description": "Repository for espa√±ol team"},
            {"user": "Jo≈æe ≈†mit", "role": "developer"},
            {"patterns": ["*.jar", "*.war"], "name": "√©quipe-fran√ßaise"},
            {"contentClass": "any", "name": "t√´am-√±ame"},
        ]

        for test_data in test_cases:
            json_str = json.dumps(test_data)

            # Should encode with UTF-8 (the fix)
            encoded_utf8 = json_str.encode(encoding="utf-8")
            assert isinstance(encoded_utf8, bytes)

            # Verify it can be decoded back
            decoded = encoded_utf8.decode("utf-8")
            assert decoded == json_str

            # For data with non-latin-1 characters, latin-1 encoding should fail
            if any(char in json_str for char in ["≈°", "≈†", "√±", "√©", "√´"]):
                try:
                    json_str.encode("latin-1")
                except UnicodeEncodeError:
                    pass  # Expected for characters outside latin-1

    def test_nexus_target_creation_with_unicode(self):
        """Test that Nexus target creation handles Unicode in names."""
        import json

        # Simulate creating a Nexus target with Unicode in the name
        target = {
            "data": {
                "contentClass": "any",
                "patterns": ["*.jar"],
                "name": "√©quipe-d√©veloppement",
            }
        }

        # Test with ensure_ascii=False to preserve Unicode characters
        json_data = json.dumps(target, ensure_ascii=False).encode(encoding="utf-8")
        assert isinstance(json_data, bytes)

        # Verify the data can be decoded
        decoded = json_data.decode("utf-8")
        assert "√©quipe-d√©veloppement" in decoded

    def test_nexus_user_creation_with_unicode(self):
        """Test that Nexus user creation handles Unicode names."""
        import json

        # Simulate creating a Nexus user with Unicode characters
        user_data = {
            "data": {
                "userId": "fm√ºller",
                "firstName": "Fran√ßois",
                "lastName": "M√ºller",
                "email": "fmuller@example.com",
                "roles": ["developer"],
            }
        }

        # Test with ensure_ascii=False to preserve Unicode characters
        json_data = json.dumps(user_data, ensure_ascii=False).encode(encoding="utf-8")
        assert isinstance(json_data, bytes)

        decoded = json_data.decode("utf-8")
        assert "Fran√ßois" in decoded
        assert "M√ºller" in decoded

    def test_nexus_role_creation_with_unicode(self):
        """Test that Nexus role creation handles Unicode descriptions."""
        import json

        role_data = {
            "data": {
                "id": "dev-team",
                "name": "Development Team",
                "description": "√âquipe de d√©veloppement fran√ßais",
                "privileges": ["read", "write"],
            }
        }

        # Test with ensure_ascii=False to preserve Unicode characters
        json_data = json.dumps(role_data, ensure_ascii=False).encode(encoding="utf-8")
        decoded = json_data.decode("utf-8")
        assert "√âquipe de d√©veloppement fran√ßais" in decoded

    def test_nexus_repo_group_update_with_unicode(self):
        """Test that Nexus repo group updates handle Unicode."""
        import json

        repo_data = {"data": {"name": "Grupo de reposit√≥rios", "description": "Repository group for espa√±ol projects"}}

        # Test with ensure_ascii=False to preserve Unicode characters
        json_data = json.dumps(repo_data, ensure_ascii=False).encode(encoding="utf-8")
        decoded = json_data.decode("utf-8")
        assert "Grupo de reposit√≥rios" in decoded
        assert "espa√±ol" in decoded
